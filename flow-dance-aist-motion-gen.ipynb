{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k_4te8787fF"
      },
      "source": [
        "#### Music-conditioned human motion generation using AIST++ dataset\n",
        "\n",
        "AIST++ dataset contains 1408 sequences of 3D human dance motion, each sequence with duration 7-48 seconds, along with corresponding music.\n",
        "\n",
        "Task: Flow-matching for generating realistic human dance motion conditioned on a music segment, and potentially on a seed motion sequence.\n",
        "\n",
        "Input: Segment of music -> embed with some pretrained model\n",
        "\n",
        "Output: Sequence of 3D Human body keypoints data of shape (N frames x 17 joints x 3 coordinates) following COCO format at 60Fps. (sample visualization below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUHOWSS987fH"
      },
      "source": [
        "<img src=\"https://github.com/thelmn/flow-diffusion/blob/master/aistpp-dataset-dance-screen.png?raw=1\" width=\"350\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-6pWEEo87fH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from pathlib import Path\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTchE3ru87fI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IQsas7h87fJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B-w0lmF87fK"
      },
      "outputs": [],
      "source": [
        "# download AIST++ keypoints3d data from https://google.github.io/aistplusplus_dataset/download.html\n",
        "# download AIST++ music dataset from https://github.com/Garfield-kh/TM2D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HOME_PATH = Path(\"data/aist_plusplus\")\n",
        "HOME_PATH = Path(\"drive/MyDrive/datasets/aist_plusplus\")\n"
      ],
      "metadata": {
        "id": "OrPS_Ef09JBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "1ZXHqe-N-Eo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re1WLmf487fK"
      },
      "outputs": [],
      "source": [
        "MUSIC_FOLDER = HOME_PATH / 'all_music'\n",
        "KEYPOINTS_FOLDER = HOME_PATH / 'keypoints3d'\n",
        "# MOTIONS_FOLDER = HOME_PATH / 'motions'\n",
        "\n",
        "music_list = sorted(glob.glob(f'{MUSIC_FOLDER}/*.wav'))\n",
        "keypoints_list = sorted(glob.glob(f'{KEYPOINTS_FOLDER}/*.pkl'))\n",
        "\n",
        "print(len(music_list))\n",
        "print(music_list[:3])\n",
        "print(len(keypoints_list))\n",
        "print(keypoints_list[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU7X4S8v87fK"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import librosa\n",
        "except ImportError:\n",
        "  !pip install librosa\n",
        "  import librosa\n",
        "\n",
        "try:\n",
        "  import pywt\n",
        "except ImportError:\n",
        "  !pip install PyWavelets\n",
        "  import pywt\n",
        "\n",
        "try:\n",
        "  import soundfile as sf\n",
        "except ImportError:\n",
        "  !pip install soundfile\n",
        "  import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPSMuyYq87fM"
      },
      "outputs": [],
      "source": [
        "with open(keypoints_list[0], 'rb') as f:\n",
        "  sample_keypoints = pickle.load(f)\n",
        "print(sample_keypoints.keys())\n",
        "print(sample_keypoints[list(sample_keypoints.keys())[0]].shape)\n",
        "print(sample_keypoints[list(sample_keypoints.keys())[1]].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IecXAGWE87fM"
      },
      "source": [
        "##### Embed (and save) music data with pretrained model (MusicFM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5m1DiKs87fN"
      },
      "outputs": [],
      "source": [
        "EMBED_MODEL_SR = 24000  # 24kHz\n",
        "KEYPOINT_FRAME_RATE = 60  # 60 fps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjabddUu87fO"
      },
      "outputs": [],
      "source": [
        "y, sr = librosa.load(music_list[0])\n",
        "y_len_s = y.shape[0] / sr\n",
        "\n",
        "print('loaded: ', music_list[0], y.shape, 'sr:', sr, 'len (sec):', y_len_s)\n",
        "\n",
        "y_resampled = librosa.resample(y, orig_sr=sr, target_sr=EMBED_MODEL_SR)\n",
        "print(y.shape, y_resampled.shape)\n",
        "# save\n",
        "sf.write(HOME_PATH/f'sample_resampled_{EMBED_MODEL_SR}.wav', y_resampled, EMBED_MODEL_SR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9ZYOd9V87fO"
      },
      "outputs": [],
      "source": [
        "# plot sample\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(y_resampled[:2*EMBED_MODEL_SR])\n",
        "plt.title('Audio sample')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys4vWJo187fO"
      },
      "outputs": [],
      "source": [
        "# load musicfm model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w_GANJh87fP"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $(pwd)/models/musicfm/data/\n",
        "!wget -P $(pwd)/models/musicfm/data/ https://huggingface.co/minzwon/MusicFM/resolve/main/msd_stats.json\n",
        "!wget -P $(pwd)/models/musicfm/data/ https://huggingface.co/minzwon/MusicFM/resolve/main/pretrained_msd.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XmMiwv087fP"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/minzwon/musicfm.git $(pwd)/models/musicfm/musicfm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ehut_UE87fQ"
      },
      "outputs": [],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "q-kAUiiZ87fQ"
      },
      "outputs": [],
      "source": [
        "MUSICFM_PATH = './models/musicfm/'\n",
        "sys.path.append(MUSICFM_PATH)\n",
        "\n",
        "from musicfm.model.musicfm_25hz import MusicFM25Hz\n",
        "\n",
        "musicfm = MusicFM25Hz(\n",
        "    is_flash=False,\n",
        "    stat_path=os.path.join(MUSICFM_PATH, \"data\", \"msd_stats.json\"),\n",
        "    model_path=os.path.join(MUSICFM_PATH, \"data\", \"pretrained_msd.pt\"),\n",
        ")\n",
        "musicfm.cuda()\n",
        "musicfm.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8rzRJtR87fQ"
      },
      "outputs": [],
      "source": [
        "# embed sample audio file\n",
        "y_resampled_t = torch.from_numpy(y_resampled.reshape(1, -1))\n",
        "y_resampled_t = y_resampled_t.cuda()\n",
        "emb = musicfm.get_latent(y_resampled_t)\n",
        "print(emb.shape)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(emb[0].detach().cpu().numpy().T, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
        "plt.title('Token embedding')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Embedding dimension')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# resample embedding frame rate from 25Hz to 60Hz to match keypoint data\n",
        "n_frame = int(y_len_s * KEYPOINT_FRAME_RATE)\n",
        "token_emb = torch.nn.AdaptiveAvgPool1d(n_frame)(emb.transpose(1, 2)).transpose(1, 2)\n",
        "print(token_emb.shape, token_emb.shape[1] / KEYPOINT_FRAME_RATE)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(token_emb[0].detach().cpu().numpy().T, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
        "plt.title('Token embedding')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Embedding dimension')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T2jCcqtY87fR"
      },
      "outputs": [],
      "source": [
        "for i, music_file in enumerate(music_list):\n",
        "  y, sr = librosa.load(music_file)\n",
        "  y_resampled = librosa.resample(y, orig_sr=sr, target_sr=EMBED_MODEL_SR)\n",
        "  y_len_s = y.shape[0] / sr\n",
        "\n",
        "  y_resampled_t = torch.from_numpy(y_resampled.reshape(1, -1))\n",
        "  y_resampled_t = y_resampled_t.cuda()\n",
        "\n",
        "  emb = musicfm.get_latent(y_resampled_t)\n",
        "  n_frame = int(y_len_s * KEYPOINT_FRAME_RATE)\n",
        "  token_emb = torch.nn.AdaptiveAvgPool1d(n_frame)(emb.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "  # save token embedding\n",
        "  music_name = os.path.basename(music_file).split('.')[0]\n",
        "  embeds_file = os.path.join(MUSIC_FOLDER, f'{music_name}_musicfm_emb_60Hz.npy')\n",
        "  np.save(embeds_file, token_emb.detach().cpu().numpy())\n",
        "\n",
        "  print(f'{i+1}/{len(music_list)}: loaded: {music_file}, {y.shape}, sr: {sr}, len (sec): {y_len_s}. \\n'\n",
        "        f'saved: {embeds_file}, token emb shape: {token_emb.shape}, {token_emb.shape[1] / KEYPOINT_FRAME_RATE} sec')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To tensorflow dataset"
      ],
      "metadata": {
        "id": "z-Hngm4cEY8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_dataset_arrs(music_folder, keypoints_folder, load_n):\n",
        "  music_files = glob.glob(os.path.join(music_folder, '*_musicfm_emb_60Hz.npy'))\n",
        "  keypoints_files = glob.glob(os.path.join(keypoints_folder, '*.pkl'))\n",
        "  print(f'Found {len(music_files)} music files and {len(keypoints_files)} keypoints files, loading {load_n} files...')\n",
        "\n",
        "  music_ids = [os.path.basename(music_file).split('_')[0]\n",
        "               for music_file in music_files]\n",
        "  music_ids = dict(zip(music_ids, range(len(music_ids))))\n",
        "\n",
        "  keypoints = []\n",
        "  music_embeds = {}\n",
        "  for keypoints_file in tqdm(keypoints_files[:load_n]):\n",
        "    filename = os.path.basename(keypoints_file).split('.')[0]\n",
        "    music_key = filename.split('_')[-2]\n",
        "    music_file = os.path.join(music_folder, f'{music_key}_musicfm_emb_60Hz.npy')\n",
        "\n",
        "    if os.path.exists(music_file):\n",
        "      with open(keypoints_file, 'rb') as f:\n",
        "        keypoints_data = pickle.load(f)\n",
        "      keypoints3d_arr = keypoints_data['keypoints3d_optim']\n",
        "      if music_ids[music_key] not in music_embeds:\n",
        "        music_embed_arr = np.load(music_file).squeeze()\n",
        "        music_embeds[music_ids[music_key]] = music_embed_arr\n",
        "        if np.isnan(music_embed_arr).any():\n",
        "          print('nan in music_embed_arr')\n",
        "      keypoints += [(music_ids[music_key], keypoints3d_arr)]\n",
        "      if np.isnan(keypoints3d_arr).any():\n",
        "        print('nan in keypoints3d_arr')\n",
        "  return music_embeds, keypoints"
      ],
      "metadata": {
        "id": "ANWtVsYuEmdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_music_embeds, sample_keypoints = load_dataset_arrs(str(MUSIC_FOLDER), str(KEYPOINTS_FOLDER), 100)\n",
        "print()\n",
        "print(len(sample_music_embeds), len(sample_keypoints))\n",
        "print(sample_keypoints[0][0], sample_music_embeds[sample_keypoints[0][0]].shape, sample_keypoints[0][1].shape)"
      ],
      "metadata": {
        "id": "CV8AgoQSS8YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "def embed_keypoints_gen(music_embeds, keypoints):\n",
        "  for music_id, keypoints3d_arr in keypoints:\n",
        "    yield (\n",
        "        keypoints3d_arr.reshape(-1, 17*3),\n",
        "        music_embeds[music_id][:keypoints3d_arr.shape[0]],\n",
        "    )\n",
        "\n",
        "sample_dataset = tf.data.Dataset.from_generator(\n",
        "    partial(embed_keypoints_gen, sample_music_embeds, sample_keypoints),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 17*3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 1024), dtype=tf.float32),\n",
        "    )\n",
        ")\n",
        "\n",
        "for sample_keypoints_arr, sample_music_embed_arr in sample_dataset.take(1):\n",
        "  print(\"Music Embedding shape:\", sample_music_embed_arr.shape)\n",
        "  print(\"Keypoints shape:\", sample_keypoints_arr.shape)\n",
        "\n",
        "sample_batch_size = 64\n",
        "sample_dataset = sample_dataset.padded_batch(\n",
        "    sample_batch_size, padded_shapes=([None, 17*3], [None, 1024]))\n",
        "\n",
        "for sample_batch_keypoints_arr, sample_batch_music_embed_arr in sample_dataset.take(1):\n",
        "  sample_batch_keypoints_arr = sample_batch_keypoints_arr.numpy()\n",
        "  sample_batch_music_embed_arr = sample_batch_music_embed_arr.numpy()\n",
        "  print(\"Batch Keypoints shape:\", sample_batch_keypoints_arr.shape)\n",
        "  print(\"Batch Music Embedding shape:\", sample_batch_music_embed_arr.shape)"
      ],
      "metadata": {
        "id": "d4lMreQ3JcdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first music dim of batch\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), sharex=True, sharey=True)\n",
        "\n",
        "ax1_plot = ax1.imshow(sample_batch_music_embed_arr[:, :, 0], cmap='coolwarm', vmin=-1,\n",
        "           vmax=1, aspect='auto')\n",
        "plt.colorbar(ax1_plot, ax=ax1)\n",
        "ax1.set_title('feat1 of batch music embeddings')\n",
        "ax1.set_xlabel('Time')\n",
        "ax1.set_ylabel('batch dim')\n",
        "\n",
        "ax2_plot = ax2.matshow(sample_batch_keypoints_arr[:, :, 0], cmap='coolwarm',\n",
        "            norm='linear',\n",
        "           vmin=np.quantile(sample_batch_keypoints_arr, 0.01),\n",
        "           vmax=np.quantile(sample_batch_keypoints_arr, 0.99), aspect='auto')\n",
        "plt.colorbar(ax2_plot, ax=ax2)\n",
        "ax2.set_xlabel('Time')\n",
        "ax2.set_title('feat1 of batch keypoints')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eL_UFSIOjgKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load all music+key\n",
        "music_embeds, keypoints = load_dataset_arrs(str(MUSIC_FOLDER),\n",
        "                                            str(KEYPOINTS_FOLDER), 200)\n",
        "len(music_embeds), len(keypoints)"
      ],
      "metadata": {
        "id": "IlX-G655ApcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split train/test by unique songs\n",
        "music_ids = list(music_embeds.keys())\n",
        "n_music = len(music_ids)\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "train_music_ids = jax.random.choice(key, jnp.array(music_ids), shape=[int(n_music*0.8)], replace=False).__array__()\n",
        "test_music_ids = np.setdiff1d(music_ids, train_music_ids)\n",
        "\n",
        "train_keypoints = [(m_id, kp_arr) for m_id, kp_arr in keypoints if m_id in train_music_ids]\n",
        "test_keypoints = [(m_id, kp_arr) for m_id, kp_arr in keypoints if m_id in test_music_ids]\n",
        "\n",
        "print(f'train: music_ids: {len(train_music_ids)}/{n_music}, n_keypoints: {len(train_keypoints)}')\n",
        "print(f'test: music_ids: {len(test_music_ids)}/{n_music}, n_keypoints: {len(test_keypoints)}')"
      ],
      "metadata": {
        "id": "p1W6Iy6oAwc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "train_epochs = 40\n",
        "epoch_steps = 100\n",
        "batch_size = 128\n",
        "\n",
        "def embed_keypoints_gen(music_embeds, keypoints):\n",
        "  for music_id, keypoints3d_arr in keypoints:\n",
        "    yield (\n",
        "        keypoints3d_arr.reshape(-1, 17*3),\n",
        "        music_embeds[music_id][:keypoints3d_arr.shape[0]],\n",
        "    )\n",
        "\n",
        "train_Xy_ds = tf.data.Dataset.from_generator(\n",
        "    partial(embed_keypoints_gen, music_embeds, train_keypoints),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 17*3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 1024), dtype=tf.float32),\n",
        "    )\n",
        ")\n",
        "test_Xy_ds = tf.data.Dataset.from_generator(\n",
        "    partial(embed_keypoints_gen, music_embeds, test_keypoints),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 17*3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 1024), dtype=tf.float32),\n",
        "    )\n",
        ")\n",
        "\n",
        "train_Xy_ds = (train_Xy_ds\n",
        "               .repeat()\n",
        "               .shuffle(500)\n",
        "               .padded_batch(\n",
        "                    batch_size, padded_shapes=([None, 17*3], [None, 1024]))\n",
        "               .take(epoch_steps * train_epochs)\n",
        "               .prefetch(tf.data.AUTOTUNE)\n",
        "               )\n",
        "test_Xy_ds = (test_Xy_ds\n",
        "               .padded_batch(\n",
        "                    batch_size, padded_shapes=([None, 17*3], [None, 1024]))\n",
        "               .prefetch(tf.data.AUTOTUNE)\n",
        "               )"
      ],
      "metadata": {
        "id": "9_8LalhBxDo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_X, batch_y = next(iter(train_Xy_ds))\n",
        "print(batch_X.shape, batch_y.shape)"
      ],
      "metadata": {
        "id": "VU_IW8lpHwAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Diffusion transformer"
      ],
      "metadata": {
        "id": "obXK7y7GENCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guided Diffusion encoder-only transformer with inputs (X, y, t) where X is the noisy input as a sequence, y is a conditioning sequence of vectors, t is the time step.\n",
        "# Implementation based on the DiT described in https://arxiv.org/abs/2212.09748 in jax/flax.nnx with a lot of modifications\n",
        "\n",
        "# Components:\n",
        "# FeedForward: Dense -> ReLU -> Dropout -> Dense -> Dropout\n",
        "# TransformerEncoderBlock: in -> LayerNorm -> MultiHeadSelfAttention -> Dropout -> Add(input) -> LayerNorm -> FeedForward -> Add(input) -> out\n",
        "# ConditionalTransformerEncoderBlock:\n",
        "#   (in_X, in_y_emb, t_sin_enc) -> \\\n",
        "#     - t_sin_enc -> Dense -> (t_u, t_s)\n",
        "#     - in_X -> LayerNorm -> ScaleShift(_, t_u, t_s) -> MultiHeadSelfAttention -> Dropout -> Add(_, in_X) -> \\\n",
        "#       -> LayerNorm -> ScaleShift(_, t_u, t_s) -> MultiHeadCrossAttention(_, in_y_emb) -> Dropout -> Add(_, in_X) -> \\\n",
        "#         -> LayerNorm -> ScaleShift(_, t_u, t_s) -> FeedForward -> Add(_, in_X) -> out_X\n",
        "#     -> out_X\n",
        "# TransformerEncoder:\n",
        "#   (X, y, t) -> \\\n",
        "#     - t -> get_sinusoidal_encodings(t) -> t_sin_enc\n",
        "#     - y -> Dense -> PositionalEncoding -> [ TransformerEncoderBlock x n ] -> LayerNorm -> y_emb\n",
        "#     - X -> Dense -> PositionalEncoding -> [ ConditionalTransformerEncoderBlock(_, y_emb, t_sin_enc) x n ] -> LayerNorm -> Dense -> out_X\n",
        "#     -> out_X\n",
        "\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "from flax import nnx\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "class FeedForward(nnx.Module):\n",
        "  def __init__(self, *, in_feats, out_feats=None, hidden_feats=None, dropout=0.0, rngs: nnx.Rngs):\n",
        "    out_feats = out_feats or in_feats\n",
        "    hidden_feats = hidden_feats or in_feats\n",
        "    self.dense1 = nnx.Linear(in_feats, hidden_feats, rngs=rngs)\n",
        "    self.dense2 = nnx.Linear(hidden_feats, out_feats, rngs=rngs)\n",
        "    self.dropout = nnx.Dropout(rate=dropout, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x, train: bool = False):\n",
        "    x = self.dense1(x)\n",
        "    x = nnx.relu(x)\n",
        "    x = self.dropout(x, deterministic=not train)\n",
        "    x = self.dense2(x)\n",
        "    x = self.dropout(x, deterministic=not train)\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_sinusoidal_encodings(t, embedding_dim):\n",
        "  half_dim = embedding_dim // 2\n",
        "  emb = jnp.log(embedding_dim) / (half_dim - 1)\n",
        "  emb = jnp.exp(jnp.arange(half_dim) * -emb)\n",
        "  emb = half_dim * t * emb[None, :]\n",
        "  emb = jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], axis=-1)\n",
        "  return emb\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nnx.Module):\n",
        "  def __init__(self, *, in_feats, num_heads=8, dropout=0.0, ff_feats_x=4, rngs: nnx.Rngs):\n",
        "    self.attention = nnx.MultiHeadAttention(\n",
        "      num_heads=num_heads,\n",
        "      in_features=in_feats,\n",
        "      out_features=in_feats,\n",
        "      dropout_rate=dropout,\n",
        "      decode=False,\n",
        "      deterministic=True,\n",
        "      rngs=rngs)\n",
        "    self.ff = FeedForward(in_feats=in_feats, hidden_feats=in_feats * ff_feats_x, dropout=dropout, rngs=rngs)\n",
        "    self.layer_norm1 = nnx.LayerNorm(in_feats, rngs=rngs)\n",
        "    self.layer_norm2 = nnx.LayerNorm(in_feats, rngs=rngs)\n",
        "    self.dropout = nnx.Dropout(rate=dropout, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x, mask=None, train: bool = False):\n",
        "    x = self.layer_norm1(x)\n",
        "    x = self.attention(x, mask=mask)\n",
        "    x = self.dropout(x, deterministic=not train)\n",
        "    x = x + x\n",
        "    x = self.layer_norm2(x)\n",
        "    x = self.ff(x, train=train)\n",
        "    x = x + x\n",
        "    return x\n",
        "\n",
        "\n",
        "class ConditionalTransformerEncoderBlock(nnx.Module):\n",
        "  def __init__(self, *, in_feats, t_feats, num_heads=8, dropout=0.0, ff_feats_x=4, rngs: nnx.Rngs):\n",
        "    self.attention = nnx.MultiHeadAttention(\n",
        "      num_heads=num_heads,\n",
        "      in_features=in_feats,\n",
        "      out_features=in_feats,\n",
        "      dropout_rate=dropout,\n",
        "      decode=False,\n",
        "      deterministic=True,\n",
        "      rngs=rngs)\n",
        "    self.cross_attention = nnx.MultiHeadAttention(\n",
        "      num_heads=num_heads,\n",
        "      in_features=in_feats,\n",
        "      out_features=in_feats,\n",
        "      dropout_rate=dropout,\n",
        "      decode=False,\n",
        "      deterministic=True,\n",
        "      rngs=rngs)\n",
        "    self.ff = FeedForward(in_feats=in_feats, hidden_feats=in_feats * ff_feats_x, dropout=dropout, rngs=rngs)\n",
        "    self.layer_norm1 = nnx.LayerNorm(in_feats, rngs=rngs)\n",
        "    self.layer_norm2 = nnx.LayerNorm(in_feats, rngs=rngs)\n",
        "    self.layer_norm3 = nnx.LayerNorm(in_feats, rngs=rngs)\n",
        "    self.dropout = nnx.Dropout(rate=dropout, rngs=rngs)\n",
        "    self.dense_t_u_s = nnx.Linear(t_feats, in_feats * 2, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x, y_enc, t_sin_enc, mask=None, train: bool = False):\n",
        "    # x: (batch_size, seq_len, in_feats)\n",
        "    # y_enc: (batch_size, seq_len, in_feats)\n",
        "    # t_sin_enc: (batch_size, t_feats)\n",
        "    # mask: (batch_size, seq_len, seq_len)\n",
        "\n",
        "    t_u_s = self.dense_t_u_s(t_sin_enc)[:, None, :]  # (batch_size, 1, in_feats * 2)\n",
        "    t_u, t_s = jnp.split(t_u_s, 2, axis=-1)\n",
        "    #\n",
        "    x = self.layer_norm1(x)\n",
        "    x = (x + t_u) * t_s\n",
        "    x = self.attention(x, mask=mask)\n",
        "    x = self.dropout(x, deterministic=not train)\n",
        "    x = x + x\n",
        "    #\n",
        "    x = self.layer_norm2(x)\n",
        "    x = (x + t_u) * t_s\n",
        "    x = self.cross_attention(x, y_enc, mask=mask)\n",
        "    x = self.dropout(x, deterministic=not train)\n",
        "    x = x + x\n",
        "    #\n",
        "    x = self.layer_norm3(x)\n",
        "    x = (x + t_u) * t_s\n",
        "    x = self.ff(x, train=train)\n",
        "    x = x + x\n",
        "    return x\n",
        "\n",
        "\n",
        "class AddPositionalEncoding(nnx.Module):\n",
        "  def __init__(self, *, max_len=512, in_feats=64, rngs: nnx.Rngs):\n",
        "    self.max_len = max_len\n",
        "    self.in_feats = in_feats\n",
        "    self.rngs = rngs\n",
        "    self.pos_enc = get_sinusoidal_encodings((jnp.arange(max_len)/max_len).reshape(-1, 1), in_feats)\n",
        "    self.pos_enc = jnp.expand_dims(self.pos_enc, axis=0)  # (1, max_len, in_feats)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # x: (batch_size, seq_len, in_feats)\n",
        "    # self.pos_enc: (1, max_len, in_feats)\n",
        "    # return x + self.pos_enc[:, :x.shape[1], :]\n",
        "    return x + self.pos_enc[:, :x.shape[1], :]\n",
        "\n",
        "\n",
        "class GuidedDiff_TransfomerEncoder(nnx.Module):\n",
        "  def __init__(self, *,\n",
        "               in_feats_X, in_feats_y,\n",
        "               Xy_feats, out_feats, t_feats=64,\n",
        "               num_heads=8, num_blocks_X=4, num_blocks_y=2,\n",
        "               dropout=0.0, ff_feats_x=4, max_len=1024,\n",
        "               padding_max_value=0,\n",
        "               rngs: nnx.Rngs):\n",
        "    self.get_masked_timesteps = lambda y: (y != padding_max_value).sum(axis=-1) > 0\n",
        "    self.dense_X = nnx.Linear(in_feats_X, Xy_feats, rngs=rngs)\n",
        "    self.dense_y = nnx.Linear(in_feats_y, Xy_feats, rngs=rngs)\n",
        "    self.layer_norm_out_X = nnx.LayerNorm(Xy_feats, rngs=rngs)\n",
        "    self.layer_norm_out_y = nnx.LayerNorm(Xy_feats, rngs=rngs)\n",
        "    self.get_sin_enc = partial(get_sinusoidal_encodings, embedding_dim=t_feats)\n",
        "    self.pos_enc_X = AddPositionalEncoding(max_len=max_len, in_feats=Xy_feats, rngs=rngs)\n",
        "    self.pos_enc_y = AddPositionalEncoding(max_len=max_len, in_feats=Xy_feats, rngs=rngs)\n",
        "    self.encoders_y = nnx.Sequential(*[\n",
        "      TransformerEncoderBlock(in_feats=Xy_feats, num_heads=num_heads,\n",
        "                              dropout=dropout, ff_feats_x=ff_feats_x, rngs=rngs)\n",
        "      for _ in range(num_blocks_y)\n",
        "    ])\n",
        "    self.encoders_X = [ConditionalTransformerEncoderBlock(\n",
        "      in_feats=Xy_feats, t_feats=t_feats, num_heads=num_heads,\n",
        "      dropout=dropout, ff_feats_x=ff_feats_x, rngs=rngs)\n",
        "      for _ in range(num_blocks_X)]\n",
        "    self.dense_out = nnx.Linear(Xy_feats, out_feats, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x, y, t, train: bool = False):\n",
        "    # x: (batch_size, seq_len_X, in_feats_X)\n",
        "    # y: (batch_size, seq_len_y, in_feats_y)\n",
        "    # t: (batch_size,)\n",
        "    # mask inputs where all y values are padding_max_value\n",
        "\n",
        "    # mask = flax.linen.make_attention_mask(inputs != 0, inputs != 0)\n",
        "    y_masked = self.get_masked_timesteps(y)\n",
        "    mask = flax.linen.make_attention_mask(\n",
        "      y_masked, y_masked, dtype=jnp.float32\n",
        "    )\n",
        "\n",
        "    x = self.dense_X(x)\n",
        "    x = self.pos_enc_X(x)\n",
        "\n",
        "    y = self.dense_y(y)\n",
        "    y = self.pos_enc_y(y)\n",
        "    y = self.encoders_y(y, mask=mask, train=train)\n",
        "    y = self.layer_norm_out_y(y)\n",
        "\n",
        "    t = t.reshape(-1, 1)\n",
        "    t_sin_enc = self.get_sin_enc(t)\n",
        "\n",
        "    for encoder in self.encoders_X:\n",
        "      x = encoder(x, y, t_sin_enc, mask=mask, train=train)\n",
        "\n",
        "    x = self.layer_norm_out_X(x)\n",
        "\n",
        "    # output -> Dense\n",
        "    x = self.dense_out(x)\n",
        "\n",
        "    # mask output timesteps\n",
        "    x = x * y_masked[:, :, None]\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "mWujbr0mEIkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model\n",
        "\n",
        "sample_batch_size = 3\n",
        "sample_max_sequence_length = 4\n",
        "sample_feats_X = 6\n",
        "sample_feats_y = 4\n",
        "sample_feats_Xy = 10\n",
        "sample_feats_out = 2\n",
        "sample_t_feats = 16\n",
        "\n",
        "key = jax.random.PRNGKey(2)\n",
        "X_sample = jax.random.normal(key, (sample_batch_size, sample_max_sequence_length, sample_feats_X))\n",
        "y_sample = jax.random.normal(key, (sample_batch_size, sample_max_sequence_length, sample_feats_y))\n",
        "t_sample = jax.random.uniform(key, (sample_batch_size,))\n",
        "\n",
        "# mask some y time steps\n",
        "y_sample_len = jax.random.randint(key, (sample_batch_size,), minval=1, maxval=sample_max_sequence_length + 1)\n",
        "y_sample_mask = jnp.arange(sample_max_sequence_length)[None, :] < y_sample_len[:, None]\n",
        "y_sample = jnp.where(y_sample_mask[:, :, None], y_sample, 0.0)\n",
        "\n",
        "X_sample.shape, y_sample.shape, t_sample.shape\n"
      ],
      "metadata": {
        "id": "wNLGEVMWEJ59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y_sample\n",
        "y_masked = (y != 0).sum(axis=-1) > 0\n",
        "mask = flax.linen.make_attention_mask(\n",
        "  y_masked, y_masked, dtype=jnp.float32\n",
        ")\n",
        "mask"
      ],
      "metadata": {
        "id": "trPUFgACeHiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_masked"
      ],
      "metadata": {
        "id": "6-JVn_AnFrER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model = GuidedDiff_TransfomerEncoder(\n",
        "  in_feats_X=sample_feats_X, in_feats_y=sample_feats_y,\n",
        "  Xy_feats=sample_feats_Xy, out_feats=sample_feats_out, t_feats=64,\n",
        "  num_heads=2, num_blocks_X=2, num_blocks_y=2,\n",
        "  dropout=0.0, ff_feats_x=4, max_len=sample_max_sequence_length,\n",
        "  padding_max_value=0,\n",
        "  rngs=nnx.Rngs(42),\n",
        ")\n",
        "nnx.display(sample_model)"
      ],
      "metadata": {
        "id": "GSt4uHvOeHlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_sample_out = sample_model(X_sample, y_sample, t_sample)\n",
        "X_sample_out.shape"
      ],
      "metadata": {
        "id": "muL4v2VjeHnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_sample_out"
      ],
      "metadata": {
        "id": "WHmKvSz1FxFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_X.shape, batch_y.shape"
      ],
      "metadata": {
        "id": "sIuCtRNZvlp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init actual model\n",
        "\n",
        "hidden_Xy_dim = 128\n",
        "num_heads = 8\n",
        "num_blocks_X = 4\n",
        "num_blocks_y = 2\n",
        "dropout = 0.1\n",
        "ff_feats_x = 4\n",
        "max_len = batch_X.shape[1]\n",
        "\n",
        "model = GuidedDiff_TransfomerEncoder(\n",
        "  in_feats_X=batch_X.shape[-1], in_feats_y=batch_y.shape[-1],\n",
        "  Xy_feats=hidden_Xy_dim, out_feats=batch_X.shape[-1], t_feats=64,\n",
        "  num_heads=num_heads, num_blocks_X=num_blocks_X, num_blocks_y=num_blocks_y,\n",
        "  dropout=dropout, ff_feats_x=ff_feats_x, max_len=max_len,\n",
        "  padding_max_value=0,\n",
        "  rngs=nnx.Rngs(42),\n",
        ")\n",
        "nnx.display(model)"
      ],
      "metadata": {
        "id": "TkOVzo1KvYgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_X_pred = model(batch_X.numpy(), batch_y.numpy(), jnp.ones(batch_X.shape[0]))\n",
        "batch_X_pred.shape"
      ],
      "metadata": {
        "id": "OzJesy2gLk7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Guided conditional flow matching loss"
      ],
      "metadata": {
        "id": "P2bOAgHIuBmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "\n",
        "def guided_cfm_loss_fn(model, batch_z, batch_y, batch_t, batch_e, train):\n",
        "  \"\"\"Compute the conditional flow matching loss.\"\"\"\n",
        "  # add noise to the input\n",
        "  alpha_t = batch_t\n",
        "  beta_t = (1 - alpha_t)\n",
        "  batch_x = alpha_t * batch_z + beta_t * batch_e\n",
        "  pred_e = model(batch_x, batch_y, batch_t, train=train)\n",
        "  target_e = (batch_z - batch_e)\n",
        "  loss = optax.losses.squared_error(pred_e, target_e).mean()\n",
        "  return loss\n",
        "\n",
        "@nnx.jit\n",
        "def train_step_guided(model, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch_z, batch_y, key):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  # sample t: [batch_size,1,1,1] ~ Unif[0,1] and e: batch_shape ~ N(0,1)\n",
        "  batch_t = jax.random.uniform(key, [batch_z.shape[0]] + ([1]*(batch_z.ndim-1)))\n",
        "  batch_e = jax.random.normal(key, batch_z.shape)\n",
        "\n",
        "  loss, grads = nnx.value_and_grad(guided_cfm_loss_fn, argnums=0)(model, batch_z, batch_y, batch_t, batch_e, train=True)\n",
        "  metrics.update(loss=loss)  # In-place updates.\n",
        "  optimizer.update(grads)  # In-place updates.\n",
        "\n",
        "@nnx.jit\n",
        "def eval_step_guided(model, metrics: nnx.MultiMetric, batch_z, batch_y, key):\n",
        "  # sample t: [batch_size,1,1,1] ~ Unif[0,1] and e: batch_shape ~ N(0,1)\n",
        "  batch_t = jax.random.uniform(key, [batch_z.shape[0]] + ([1]*(batch_z.ndim-1)))\n",
        "  batch_e = jax.random.normal(key, batch_z.shape)\n",
        "\n",
        "  loss = guided_cfm_loss_fn(model, batch_z, batch_y, batch_t, batch_e, train=False)\n",
        "  metrics.update(loss=loss)  # In-place updates."
      ],
      "metadata": {
        "id": "ODVRVuJYeHqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ],
      "metadata": {
        "id": "bi4CuOHEvP-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-2\n",
        "momentum = 0.9\n",
        "\n",
        "# optax.schedules.cosine_onecycle_schedule(transition_steps: int, peak_value: float, pct_start: float = 0.3, div_factor: float = 25.0, final_div_factor: float = 10000.0)\n",
        "lr_schedule = optax.schedules.cosine_onecycle_schedule(\n",
        "  (25)*epoch_steps, peak_value=learning_rate,\n",
        "  pct_start=5/train_epochs, div_factor=1000, final_div_factor=10.0)\n",
        "\n",
        "optimizer = nnx.Optimizer(\n",
        "  model,\n",
        "  optax.inject_hyperparams(optax.adamw)(lr_schedule, momentum, weight_decay=1e-7),\n",
        ")\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        ")\n",
        "\n",
        "nnx.display(optimizer)\n"
      ],
      "metadata": {
        "id": "y1UMC-xNvJTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}